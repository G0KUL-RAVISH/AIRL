{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "EpQeLksK9gvx",
        "outputId": "1a4719cc-e034-4f90-f18e-3b82260c8183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.56.1 accelerate timm opencv-python pycocotools\n",
        "\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything-2.git\n",
        "\n",
        "!pip install -q hydra-core==1.3.2 omegaconf==2.3.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgGs5Tyh-vL8",
        "outputId": "122e30d2-abb7-41c9-d784-f406932fb1d8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P /content/drive/MyDrive/models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTuW4_Cp-6iI",
        "outputId": "33fc3684-925d-4df0-8c93-db00d61f19ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-04 17:41:48--  https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.165.75.95, 3.165.75.66, 3.165.75.59, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.165.75.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 897952466 (856M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘/content/drive/MyDrive/models/sam2_hiera_large.pt’\n",
            "\n",
            "sam2_hiera_large.pt 100%[===================>] 856.35M   105MB/s    in 5.4s    \n",
            "\n",
            "2025-10-04 17:41:54 (158 MB/s) - ‘/content/drive/MyDrive/models/sam2_hiera_large.pt’ saved [897952466/897952466]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2/sam2_hiera_l.yaml -P /content/models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cldGCMgw_V6S",
        "outputId": "ed6628ff-2a02-4e78-e7c7-979ae340530f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-04 17:41:54--  https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2/sam2_hiera_l.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3696 (3.6K) [text/plain]\n",
            "Saving to: ‘/content/models/sam2_hiera_l.yaml’\n",
            "\n",
            "\rsam2_hiera_l.yaml     0%[                    ]       0  --.-KB/s               \rsam2_hiera_l.yaml   100%[===================>]   3.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-04 17:41:54 (64.8 MB/s) - ‘/content/models/sam2_hiera_l.yaml’ saved [3696/3696]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from transformers import (\n",
        "    AutoProcessor, AutoModelForZeroShotObjectDetection,\n",
        "    OwlViTProcessor, OwlViTForObjectDetection,\n",
        "    CLIPProcessor, CLIPModel\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Grounding DINO (primary detector)\n",
        "DINO_ID = \"IDEA-Research/grounding-dino-base\"\n",
        "dino_processor = AutoProcessor.from_pretrained(DINO_ID)\n",
        "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(DINO_ID).to(device).eval()\n",
        "\n",
        "#OWL-ViT (fallback detector)\n",
        "OWL_ID = \"google/owlvit-base-patch32\"\n",
        "owl_processor = OwlViTProcessor.from_pretrained(OWL_ID)\n",
        "owl_model = OwlViTForObjectDetection.from_pretrained(OWL_ID).to(device).eval()\n",
        "\n",
        "#CLIP (re-ranker)\n",
        "CLIP_ID = \"openai/clip-vit-base-patch32\"\n",
        "clip_processor = CLIPProcessor.from_pretrained(CLIP_ID)\n",
        "clip_model = CLIPModel.from_pretrained(CLIP_ID).to(device).eval()\n",
        "\n",
        "#SAM 2 (segmentation)\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "\n",
        "# Set these to your uploaded files / paths:\n",
        "SAM2_CKPT = \"/content/drive/MyDrive/models/sam2_hiera_large.pt\"\n",
        "SAM2_CFG  = \"sam2_hiera_l.yaml\"\n",
        "\n",
        "assert os.path.exists(SAM2_CKPT), f\"SAM2 checkpoint not found at {SAM2_CKPT}\"\n",
        "\n",
        "# Build SAM 2 model. IMPORTANT: pass config NAME, not a file path.\n",
        "_sam2_model = build_sam2(SAM2_CFG, ckpt_path=SAM2_CKPT, device=device, mode=\"eval\")\n",
        "sam2_image  = SAM2ImagePredictor(_sam2_model)\n",
        "\n",
        "print(\"Models ready (DINO, OWL-ViT, CLIP, SAM 2).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6uQrsBJ-yqL",
        "outputId": "782453a2-6cf5-4630-d073-81fe667d474f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models ready (DINO, OWL-ViT, CLIP, SAM 2).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "def detect_objects(image_pil: Image.Image,\n",
        "                   text_prompt: str,\n",
        "                   dino_score_thresh: float = 0.30,\n",
        "                   owl_score_thresh: float = 0.30) -> List[Tuple[list, float]]:\n",
        "    \"\"\"\n",
        "    Returns list of (box_xyxy, score) in pixel coordinates.\n",
        "    Grounding DINO first; if none above threshold, fallback to OWL-ViT.\n",
        "    \"\"\"\n",
        "    W, H = image_pil.size\n",
        "\n",
        "    #Grounding DINO\n",
        "    query = text_prompt.strip().lower()\n",
        "    if not query.endswith(\".\"):\n",
        "        query += \".\"\n",
        "\n",
        "    dinputs = dino_processor(images=image_pil, text=[query], return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        douts = dino_model(**dinputs)\n",
        "\n",
        "    # transformers 4.56.1: only pass outputs + target_sizes (+ input_ids to map labels)\n",
        "    dres = dino_processor.post_process_grounded_object_detection(\n",
        "        outputs=douts,\n",
        "        input_ids=dinputs.input_ids,             # lets HF derive text_labels\n",
        "        target_sizes=[(H, W)]                    # pixel boxes\n",
        "    )\n",
        "\n",
        "    dets = []\n",
        "    if dres and len(dres) > 0:\n",
        "        boxes = dres[0].get(\"boxes\", [])\n",
        "        scores = dres[0].get(\"scores\", [])\n",
        "        for b, s in zip(boxes, scores):\n",
        "            s = float(s)\n",
        "            if s >= dino_score_thresh:\n",
        "                dets.append(([float(x) for x in b.tolist()], s))\n",
        "\n",
        "    #Fallback: OWL-ViT\n",
        "    if len(dets) == 0:\n",
        "        oinputs = owl_processor(text=[[text_prompt]], images=image_pil, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            oouts = owl_model(**oinputs)\n",
        "\n",
        "        target = torch.tensor([(H, W)], device=device)\n",
        "        ores = owl_processor.post_process(outputs=oouts, target_sizes=target)\n",
        "        if ores and len(ores) > 0:\n",
        "            oboxes = ores[0].get(\"boxes\", [])\n",
        "            oscores = ores[0].get(\"scores\", [])\n",
        "            for b, s in zip(oboxes, oscores):\n",
        "                s = float(s)\n",
        "                if s >= owl_score_thresh:\n",
        "                    dets.append(([float(x) for x in b.tolist()], s))\n",
        "\n",
        "    return dets\n"
      ],
      "metadata": {
        "id": "XLH_mfu_AvKL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_detections_by_clip(image_pil: Image.Image,\n",
        "                              detections: list,\n",
        "                              text_prompt: str,\n",
        "                              keep_ratio: float = 0.90):\n",
        "    \"\"\"\n",
        "    detections: list of (box_xyxy, score). Returns list of boxes (xyxy) kept by CLIP re-rank.\n",
        "    \"\"\"\n",
        "    if not detections:\n",
        "        return []\n",
        "\n",
        "    # crop each box\n",
        "    crops = []\n",
        "    for (box, _score) in detections:\n",
        "        x0, y0, x1, y1 = map(int, box)\n",
        "        x0 = max(0, x0); y0 = max(0, y0)\n",
        "        x1 = min(image_pil.width, x1); y1 = min(image_pil.height, y1)\n",
        "        crops.append(image_pil.crop((x0, y0, x1, y1)))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        t = clip_processor(text=[text_prompt], return_tensors=\"pt\", padding=True).to(device)\n",
        "        i = clip_processor(images=crops, return_tensors=\"pt\").to(device)\n",
        "        tfeat = clip_model.get_text_features(**t)           # (1, d)\n",
        "        ifeat = clip_model.get_image_features(**i)          # (N, d)\n",
        "        tfeat = torch.nn.functional.normalize(tfeat, dim=-1)\n",
        "        ifeat = torch.nn.functional.normalize(ifeat, dim=-1)\n",
        "        sims = (ifeat @ tfeat.T).squeeze(-1).detach().cpu().numpy()  # (N,)\n",
        "\n",
        "    top = float(np.max(sims))\n",
        "    keep_idx = [k for k, s in enumerate(sims) if s >= keep_ratio * top]\n",
        "    return [detections[k][0] for k in keep_idx]  # just boxes\n"
      ],
      "metadata": {
        "id": "FAGaheqxAx1I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_masks(image_pil: Image.Image, boxes_xyxy: list):\n",
        "    \"\"\"\n",
        "    SAM 2 segmentation for a list of boxes.\n",
        "    Returns: list of boolean masks (H, W)\n",
        "    \"\"\"\n",
        "    img = np.array(image_pil)\n",
        "    sam2_image.set_image(img)\n",
        "\n",
        "    H, W = img.shape[:2]\n",
        "    masks = []\n",
        "    for box in boxes_xyxy:\n",
        "        x0, y0, x1, y1 = [float(v) for v in box]\n",
        "        x0 = max(0, x0); y0 = max(0, y0)\n",
        "        x1 = min(W - 1, x1); y1 = min(H - 1, y1)\n",
        "        box_np = np.array([x0, y0, x1, y1], dtype=np.float32)\n",
        "\n",
        "        # SAM 2 image predictor; returns (masks, scores, logits/extra)\n",
        "        m, scores, _ = sam2_image.predict(box=box_np, multimask_output=False)\n",
        "        masks.append(m[0].astype(bool))\n",
        "    return masks\n"
      ],
      "metadata": {
        "id": "5mNzgkQQA0J6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_object_in_image(image_pil, text_prompt):\n",
        "    # 1) detect\n",
        "    detections = detect_objects(image_pil, text_prompt)\n",
        "    if not detections:\n",
        "        print(f\"No '{text_prompt}' found in the image.\")\n",
        "        return None, None\n",
        "\n",
        "    # 2) CLIP re-rank\n",
        "    selected_boxes = filter_detections_by_clip(image_pil, detections, text_prompt)\n",
        "    if not selected_boxes:\n",
        "        print(f\"CLIP filtering removed all detections for '{text_prompt}'.\")\n",
        "        return None, None\n",
        "\n",
        "    # 3) SAM 2 segmentation\n",
        "    masks = segment_masks(image_pil, selected_boxes)\n",
        "\n",
        "    # 4) combine + apply mask\n",
        "    final_mask = np.zeros((image_pil.height, image_pil.width), dtype=bool)\n",
        "    for m in masks:\n",
        "        final_mask |= m\n",
        "\n",
        "    image_np = np.array(image_pil)\n",
        "    output = np.zeros_like(image_np)\n",
        "    output[final_mask] = image_np[final_mask]\n",
        "    return Image.fromarray(output), final_mask\n"
      ],
      "metadata": {
        "id": "0x9Z5knqA2R4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "image_url = \"https://i.ytimg.com/vi/xH5DFu_eLUY/maxresdefault.jpg\"\n",
        "prompt = \"tree\"\n",
        "\n",
        "image_pil = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
        "result_image, mask = segment_object_in_image(image_pil, prompt)\n",
        "\n",
        "if result_image is not None:\n",
        "    result_image.save(\"output.png\")\n",
        "    print(\"Segmentation completed. Saved: output.png\")\n",
        "else:\n",
        "    print(\"No result.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5n1zkhTA4UO",
        "outputId": "ab417974-056e-48f8-c381-3a2d934034e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation completed. Saved: output.png\n"
          ]
        }
      ]
    }
  ]
}